{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 0. IMPORT LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import OneHotEncoder      \n",
    "from sklearn.model_selection import KFold   \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib \n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 1. LOOK AT THE BIG PICTURE (DONE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset's Source:\n",
    "We got this dataset from: https://www.visualcrossing.com/. \n",
    "\n",
    "Visual Crossing is considered a reputable source for weather data for several reasons:\n",
    "- Comprehensive Coverage: It offers extensive historical and forecast data for various locations, ensuring users have access to accurate information.\n",
    "\n",
    "- User-Friendly API: The well-documented API allows easy integration into applications, making it popular among developers.\n",
    "\n",
    "- Accuracy: Data is sourced from reputable meteorological organizations, ensuring high reliability and quality.\n",
    "\n",
    "- Positive Feedback: Users report satisfaction with the service, highlighting its usability and responsive support.\n",
    "\n",
    "- Industry Use: Trusted by sectors like agriculture and finance, its data is crucial for decision-making.\n",
    "\n",
    "- These factors contribute to Visual Crossing's strong reputation in the weather data industry.\n",
    "\n",
    "## 2. Data.csv Feature Explanation\n",
    "This dataset contains weather data of Hanoi (from 01/01/2020 to 20/08/2024). Each feature is described below, along with the intended analysis goals.\n",
    "\n",
    "## 3. Features Explanation:\n",
    "- **name**: The name of the location where the data was recorded (e.g., city name).\n",
    "- **datetime**: The date and time when the data was recorded.\n",
    "- **tempmax**: The maximum temperature recorded on that day.\n",
    "- **tempmin**: The minimum temperature recorded on that day.\n",
    "- **temp**: The average temperature recorded on that day.\n",
    "- **feelslikemax**: The maximum \"feels like\" temperature, which is a measure of how hot or cold it feels.\n",
    "- **feelslikemin**: The minimum \"feels like\" temperature.\n",
    "- **feelslike**: The average \"feels like\" temperature.\n",
    "- **dew**: The dew point temperature, which indicates the temperature at which air becomes saturated with moisture.\n",
    "- **humidity**: The percentage of humidity in the air.\n",
    "- **precip**: The amount of precipitation (rainfall) recorded.\n",
    "- **precipprob**: The probability of precipitation occurring.\n",
    "- **precipcover**: The percentage of the area that experienced precipitation.\n",
    "- **preciptype**: The type of precipitation (e.g., rain, snow).\n",
    "- **snow**: The amount of snowfall recorded.\n",
    "- **snowdepth**: The depth of snow accumulation.\n",
    "- **windgust**: The maximum speed of wind gusts recorded.\n",
    "- **windspeed**: The average wind speed recorded.\n",
    "- **winddir**: The direction of the wind, measured in degrees.\n",
    "- **sealevelpressure**: The atmospheric pressure at sea level.\n",
    "- **cloudcover**: The percentage of the sky covered by clouds.\n",
    "- **visibility**: The distance at which objects can be clearly seen.\n",
    "- **solarradiation**: The amount of solar radiation received, measured in W/m².\n",
    "- **solarenergy**: The amount of solar energy received, measured in MJ/m².\n",
    "- **uvindex**: The level of ultraviolet (UV) radiation.\n",
    "- **severerisk**: The risk level for severe weather events.\n",
    "- **sunrise**: The time of sunrise.\n",
    "- **sunset**: The time of sunset.\n",
    "- **moonphase**: The phase of the moon, indicating how full or new the moon is.\n",
    "- **conditions**: A brief description of the weather conditions (e.g., clear, rainy).\n",
    "- **description**: A detailed description of the weather conditions.\n",
    "- **icon**: A weather icon representing the condition.\n",
    "- **stations**: The identifiers of the weather stations that recorded the data.\n",
    "\n",
    "## 4. Output Label: \n",
    "The output label is **temp**. (from August 21, 2024, to December 31, 2024.)\n",
    "\n",
    "## 5. Analysis goals:\n",
    "Predicting temperature specifically plays a crucial role in human life by helping people make informed decisions about daily activities, energy consumption, and health management. For instance, accurate temperature forecasts allow individuals to dress appropriately for the weather, avoiding discomfort or health risks such as heatstroke in high temperatures or hypothermia in cold conditions. Businesses, especially those in industries like agriculture and construction, rely on temperature predictions to plan operations efficiently and protect their resources. Additionally, power companies use temperature forecasts to manage energy demand, ensuring that heating or cooling systems are adequately supplied. Thus, temperature predictions directly influence safety, productivity, and resource management in everyday life.\n",
    "\n",
    "## 6. Instruction on getting the csv file and set-up the environment:\n",
    "/* insert caption here*/\n",
    "\n",
    "## **7. About our process and steps:**\n",
    "Here’s a concise explanation of the steps in the script and why it's important to follow this process:\n",
    "\n",
    "1. Data Collection: Gathering accurate and relevant data is crucial for building a reliable model. Quality data leads to better insights and predictions.\n",
    "\n",
    "2. Data Preprocessing: Cleaning and preparing the data (handling missing values, encoding categorical variables) ensures that the model receives high-quality input. This step reduces noise and improves model accuracy.\n",
    "\n",
    "3. Feature Selection/Engineering: Selecting or creating relevant features helps the model focus on the most important information, enhancing its predictive power and efficiency.\n",
    "\n",
    "4. Data Normalization (using StandardScaler): Standardizing features ensures they are on the same scale, improving model performance and reducing bias towards certain features.\n",
    "\n",
    "5. Model Training: Training the model on the prepared data allows it to learn patterns and relationships, which is essential for making predictions.\n",
    "\n",
    "6. Model Evaluation: Evaluating the model's performance using metrics helps identify its effectiveness and areas for improvement, ensuring it meets the desired accuracy.\n",
    "\n",
    "7. Hyperparameter Tuning: Fine-tuning model parameters optimizes performance, allowing the model to generalize better to unseen data.\n",
    "\n",
    "8. Deployment: Deploying the model makes it accessible for real-world applications, enabling users to benefit from its predictions.\n",
    "\n",
    "Following this structured process ensures a systematic approach to building effective machine learning models, leading to reliable and actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 2. GET THE DATA (DONE). LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r'dataset/Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 3. DISCOVER THE DATA TO GAIN INSIGHTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is the function for plotting data for each year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_year_data(year):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax2 = ax.twinx()  # Create a secondary axis for pressure\n",
    "\n",
    "    # Plot temperature as a line chart on the first y-axis\n",
    "    ax.plot(monthly_avg.index, monthly_avg['temp'][year], label=f'Temp {year}', color='red')\n",
    "\n",
    "    # Plot sea level pressure as a line chart on the secondary y-axis\n",
    "    ax2.bar(monthly_avg.index, monthly_avg['humidity'][year], label=f'Humidity {year}', alpha = 0.3, color='blue')\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax.set_title(f'Temperature and Humidity in {year}')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Temperature (°C)', color='red')\n",
    "    ax2.set_ylabel('Humidity (%)', color='blue')\n",
    "    \n",
    "     # Set y-axis limit for humidity to start from 60%\n",
    "    ax2.set_ylim(60, max(monthly_avg['humidity'][year]) + 5)  # Start at 60%, leave a little space above the max value\n",
    "\n",
    "    # Set custom month labels on the x-axis\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels(months)\n",
    "\n",
    "    # Add legends outside of the plot area, further right\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.15, 1))\n",
    "    ax2.legend(loc='upper left', bbox_to_anchor=(1.15, 0.85))\n",
    "\n",
    "    # Grid for better readability\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Adjust layout and leave space for the legends\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()\n",
    "    \n",
    "def plot_temp_and_pressure(year):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax2 = ax.twinx()  # Create a secondary axis for pressure\n",
    "\n",
    "    # Plot temperature as a line chart on the first y-axis\n",
    "    ax.plot(monthly_avg.index, monthly_avg['temp'][year], label=f'Temp {year}', color='red')\n",
    "\n",
    "    # Plot sea level pressure as a line chart on the secondary y-axis\n",
    "    ax2.bar(monthly_avg.index, monthly_avg['sealevelpressure'][year], label=f'Pressure {year}', alpha = 0.3, color='blue')\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax.set_title(f'Temperature and Sea Level Pressure in {year}')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Temperature (°C)', color='red')\n",
    "    ax2.set_ylabel('Sea Level Pressure (hPa)', color='blue')\n",
    "    \n",
    "    ax2.set_ylim(985, max(monthly_avg['sealevelpressure'][year]) + 5)  # Start at 60%, leave a little space above the max value\n",
    "\n",
    "    # Set custom month labels on the x-axis\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels(months)\n",
    "\n",
    "    # Add legends outside of the plot area, further right\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.15, 1))\n",
    "    ax2.legend(loc='upper left', bbox_to_anchor=(1.15, 0.85))\n",
    "\n",
    "    # Grid for better readability\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Adjust layout and leave space for the legends\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()\n",
    "    \n",
    "def plot_humidity_vs_temp():\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Scatter plot for Temperature vs Humidity\n",
    "    ax.scatter(filtered_data['temp'], filtered_data['humidity'], color='red', label='Temperature vs Humidity', alpha=0.6)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    ax.set_xlabel('Temperature (°C)')\n",
    "    ax.set_ylabel('Humidity (%)')\n",
    "    ax.set_title('Humidity vs Temperature')\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "def plot_boxplot_temp_variables():\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Prepare the data for the box plot\n",
    "    data_to_plot = [\n",
    "        filtered_data['temp'],\n",
    "        filtered_data['feelslike'],\n",
    "        filtered_data['tempmax'],\n",
    "        filtered_data['feelslikemax'],\n",
    "        filtered_data['tempmin'],\n",
    "        filtered_data['feelslikemin']\n",
    "    ]\n",
    "\n",
    "    # Box plot for temperature variables\n",
    "    ax.boxplot(data_to_plot, patch_artist=True, notch=True, vert=True)\n",
    "\n",
    "    # Set x-axis labels and title\n",
    "    ax.set_xticklabels(['Temp', 'Feels-like', 'Temp Max', 'Feels-like Max', 'Temp Min', 'Feels-like Min'])\n",
    "    ax.set_ylabel('Temperature (°C)')\n",
    "    ax.set_title('Distribution of Temperature Variables')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "def plot_histograms_by_year():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharey=True)\n",
    "\n",
    "    years = [2020, 2021, 2022, 2023]\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, year in enumerate(years):\n",
    "        axes[i].hist(filtered_data[filtered_data['year'] == year]['temp'], bins=15, color='skyblue', edgecolor='black')\n",
    "        axes[i].set_title(f'Temperature Distribution in {year}')\n",
    "        axes[i].set_xlabel('Temperature (°C)')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Quick view of the data\n",
    "print('\\n____________ Dataset info ____________')\n",
    "print(raw_data.info())              \n",
    "print('\\n____________ Some first data examples ____________')\n",
    "print(raw_data.head(5)) \n",
    "print('\\n____________ Counts on a feature ____________')\n",
    "# print(raw_data['LEGAL DOCUMENTS'].value_counts()) \n",
    "print('\\n____________ Statistics of numeric features ____________')\n",
    "print(raw_data.describe())    \n",
    "print('\\n____________ Get specific rows and cols ____________')     \n",
    "print(raw_data.iloc[[0,1,45], [2, 5]] ) # Refer using column ID\n",
    "\n",
    "\n",
    "# Convert 'datetime' column to pandas datetime\n",
    "raw_data['datetime'] = pd.to_datetime(raw_data['datetime'])\n",
    "\n",
    "# Extract the year from the 'datetime' column into a new 'year' column\n",
    "raw_data['year'] = raw_data['datetime'].dt.year\n",
    "\n",
    "# Filter data for the years 2020 to 2024\n",
    "years = [2020, 2021, 2022, 2023, 2024]\n",
    "filtered_data = raw_data[raw_data['year'].isin(years)]\n",
    "\n",
    "# Group the data by year and month, and calculate the average temp, feels like, and precip for each month\n",
    "monthly_avg = filtered_data.groupby([filtered_data['datetime'].dt.month, filtered_data['datetime'].dt.year])[['temp', 'feelslike', 'humidity', 'sealevelpressure']].mean().unstack(1)\n",
    "\n",
    "# Define month names for the x-axis\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years[:-1]:  # Exclude 2024\n",
    "    plot_year_data(year)\n",
    "\n",
    "# Plot for the year 2024 separately\n",
    "plot_year_data(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series of graphs illustrate the relationship between **temperature** (°C) and **humidity** (%) across the months of the years **2020 to 2024**. Each graph features a red line for temperature and blue bars for humidity. The x-axis represents the months of the year, while the y-axes represent temperature on the left and humidity on the right.\n",
    "\n",
    "#### General Observations Across All Years:\n",
    "- **Temperature Trend**: Across all years, the temperature follows a similar seasonal cycle. The temperature tends to increase from the colder months (January to March), peaks around the summer months (June to July), and decreases in the latter part of the year (October to December).\n",
    "- **Humidity Trend**: Humidity generally exhibits an inverse relationship to temperature. Humidity tends to **decrease** during the hottest months (June-July) and **increase** during cooler months (November to March).\n",
    "  \n",
    "#### Year-by-Year Breakdown:\n",
    "\n",
    "##### 2020:\n",
    "- **Temperature** rises gradually from **18°C** in January to a peak of **32°C** in June before declining sharply toward the end of the year.\n",
    "- **Humidity** is highest during the winter months, reaching over **85%** in January, but decreases to **around 70%** during the summer (June to September) when temperatures peak.\n",
    "  \n",
    "##### 2021:\n",
    "- In 2021, the **temperature** pattern is consistent, with temperatures peaking at around **31°C** in July. \n",
    "- **Humidity** is relatively high early in the year, starting at about **85%** in January and declining to **75%** during the summer, before rising again in the later months.\n",
    "  \n",
    "##### 2022:\n",
    "- **Temperature** in 2022 shows a similar pattern, with peaks around **30°C** in June and July, followed by a drop toward the end of the year.\n",
    "- **Humidity** starts relatively high (above **80%**) in the early months but dips during the summer, with values dropping as low as **65%** in July.\n",
    "\n",
    "##### 2023:\n",
    "- The temperature in 2023 peaks at **30°C** in July, maintaining the same trend observed in previous years. The pattern is fairly consistent.\n",
    "- **Humidity** remains above **80%** in the early months but declines in the mid-year (around **70-75%**) during the warmest period.\n",
    "\n",
    "##### 2024:\n",
    "- In 2024, data is only available until August, but the temperature peaks at **30°C** in June and July.\n",
    "- **Humidity** hovers between **85%** in the early months and gradually decreases to around **75%** as summer approaches.\n",
    "\n",
    "#### Key Insights:\n",
    "- **Temperature and Humidity Relationship**: Across all years, an inverse relationship between temperature and humidity is evident. As **temperature increases**, **humidity decreases**. This is most noticeable during the summer months when high temperatures are accompanied by a significant drop in humidity.\n",
    "  \n",
    "- **Consistency Across Years**: The overall seasonal patterns of both temperature and humidity remain consistent across all the years. Peak temperatures occur during the summer months, and humidity is highest in the colder months and lowest during the summer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to generate the scatter plot\n",
    "plot_humidity_vs_temp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatter plot depicts the relationship between **temperature (°C)** on the x-axis and **humidity (%)** on the y-axis. The data points represent various observations of temperature and humidity, with each dot indicating a specific instance of the two variables.\n",
    "\n",
    "#### Observations:\n",
    "1. **General Trend**:\n",
    "   - As the temperature increases, humidity tends to **decrease**. This inverse relationship is particularly noticeable between temperatures ranging from **15°C to 35°C**.\n",
    "   - However, there is a **range** of temperatures (around **20°C to 30°C**) where the spread of humidity values is quite large, indicating varied humidity levels at these temperatures.\n",
    "\n",
    "2. **High Humidity at Low to Mid-Range Temperatures**:\n",
    "   - For lower temperatures (around **10°C to 20°C**), humidity remains high, often **above 70%** and frequently clustering around **80-100%**. This suggests that cooler environments generally tend to have higher humidity levels.\n",
    "   - There are only a few outlier points where humidity drops below **60%** at these lower temperatures, which might represent unusual weather conditions or specific geographic areas.\n",
    "\n",
    "3. **Convergence at High Temperatures**:\n",
    "   - Between **30°C and 35°C**, most of the humidity values converge, forming a tighter cluster. Here, humidity is primarily **below 80%**, with many data points around **60% to 70%**.\n",
    "   - The dense cluster in this region indicates that as temperatures rise, the variability in humidity decreases, and the relationship becomes more stable, converging toward lower humidity.\n",
    "\n",
    "4. **Humidity’s Inconsistency in Mid-Range Temperatures**:\n",
    "   - Between **15°C and 25°C**, there is a **wider spread** in humidity values, ranging from **40% to nearly 100%**. This suggests that temperature alone is not enough to determine humidity in this range, and other factors such as wind, rainfall, and local geography might play a significant role in affecting humidity levels.\n",
    "   - This region reflects a period where weather conditions may fluctuate more unpredictably.\n",
    "\n",
    "#### Interpretation:\n",
    "- **Inverse Relationship**: The graph suggests a general inverse relationship between temperature and humidity. As temperatures increase, the capacity of the air to hold moisture also increases, causing a decrease in relative humidity.\n",
    "- **Clustering and Spread**:\n",
    "   - At **high temperatures**, humidity is more predictable and clustered in the **60-80%** range.\n",
    "   - At **low and mid-range temperatures**, there is more variance, indicating less predictability in humidity, potentially influenced by seasonal or geographic factors.\n",
    "- **Critical Temperature Points**:\n",
    "   - Around **30°C**, the trend stabilizes, showing that high temperatures often come with lower, consistent humidity values.\n",
    "   - Around **20°C**, there is the greatest spread in humidity, making this range more volatile in terms of predicting the overall climate comfort level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data of Temperature and Sea Level Pressure\n",
    "for year in years[:-1]:  # Exclude 2024\n",
    "    plot_temp_and_pressure(year)\n",
    "\n",
    "# Plot for the year 2024 separately\n",
    "plot_temp_and_pressure(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs depict the relationship between **temperature** (°C) and **sea level pressure** (hPa) across the months of the years **2020 to 2024**. The red lines represent temperature trends over the months, while the blue bars represent the corresponding sea level pressure. Both variables are plotted against the months from **January to December** (for most years).\n",
    "\n",
    "#### General Insights:\n",
    "- The relationship between **temperature** and **sea level pressure** can be seen clearly in these graphs, where temperature shows a **strong seasonal variation**, while sea level pressure remains more **stable** with minor fluctuations throughout the year.\n",
    "- In all years, **temperature peaks** in the **summer months** (June to July) and drops sharply towards the end of the year (October to December). Conversely, **sea level pressure** tends to be **highest** in the colder months (January to March and November to December) and shows slight **dips** during the summer months.\n",
    "\n",
    "#### Temperature Patterns:\n",
    "- Across all years (2020 to 2024), **temperature exhibits a consistent seasonal cycle**, with the warmest temperatures occurring in the **mid-year months** and the coldest temperatures towards the **start and end** of the year.\n",
    "- The highest temperatures typically range between **30°C** during the summer months, while the lowest temperatures are around **18°C to 20°C** during the winter months.\n",
    "\n",
    "#### Sea Level Pressure Patterns:\n",
    "- Sea level pressure remains relatively **stable** throughout the year, usually fluctuating between **995 hPa and 1025 hPa**.\n",
    "- In each year, sea level pressure tends to be slightly higher during the **cooler months** (January to March and October to December) and slightly lower in the **warmer months** (June to August). This suggests that sea level pressure is somewhat inversely correlated with temperature.\n",
    "\n",
    "#### Year-Specific Insights:\n",
    "- **2020**: Shows a typical seasonal pattern for both temperature and pressure, with temperatures peaking around **30°C** in June and pressure peaking near **1020 hPa** during colder months.\n",
    "- **2021**: This year shows a similar trend, with slightly **lower temperatures** in the colder months (as low as **16°C**). Sea level pressure maintains a steady range, peaking in the cooler months.\n",
    "- **2022**: Similar temperature trends are observed, though with a slightly sharper dip in temperature early in the year. Sea level pressure remains consistent.\n",
    "- **2023**: The temperature pattern is in line with previous years, with a notable drop to around **18°C** at the end of the year. Sea level pressure follows the usual pattern of being stable, with only minor fluctuations.\n",
    "- **2024**: Data only extends until **August**, but the temperature pattern is similar to previous years, peaking at **30°C** during the summer, with steady sea level pressure.\n",
    "\n",
    "#### Interpretation of the Relationship Between Temperature and Sea Level Pressure:\n",
    "- **Inverse Relationship**: There is a slight **inverse relationship** between temperature and sea level pressure, where **higher temperatures** (e.g., during the summer months) tend to coincide with **lower sea level pressure** values, and **colder months** tend to have **higher sea level pressure**.\n",
    "- **Seasonal Consistency**: Both temperature and sea level pressure display seasonal consistency across all the years. The **sharp rise** and **fall** in temperatures during the summer and winter months reflect a typical climate cycle, while the more **steady nature** of sea level pressure suggests it is less impacted by seasonal changes compared to temperature.\n",
    "- **Minor Pressure Variations**: Though sea level pressure varies, these fluctuations are **relatively minor** compared to the large swings in temperature, indicating that temperature is much more sensitive to seasonal factors than pressure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to generate the box plot\n",
    "plot_boxplot_temp_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This box plot illustrates the distribution of six temperature-related variables: **Temp**, **Feels-like**, **Temp Max**, **Feels-like Max**, **Temp Min**, and **Feels-like Min**. The boxes represent the interquartile range (IQR) of each variable, with the lines extending from the boxes (whiskers) representing variability outside the upper and lower quartiles. The notches indicate the confidence interval around the median, while any circles represent outliers.\n",
    "\n",
    "#### Key Observations:\n",
    "\n",
    "1. **Temp (Average Temperature)**:\n",
    "   - The median temperature is around **25°C**, with the interquartile range (IQR) spanning from around **20°C to 30°C**.\n",
    "   - The whiskers extend from approximately **10°C to 35°C**, indicating the range of temperatures observed.\n",
    "   - There are no noticeable outliers.\n",
    "\n",
    "2. **Feels-like (Perceived Temperature)**:\n",
    "   - The perceived temperature follows a similar distribution to the actual temperature, with a median around **28°C**.\n",
    "   - The IQR is slightly wider than the actual temperature, ranging from **22°C to 35°C**.\n",
    "   - The whiskers extend from **15°C to 40°C**, indicating a broader range of perceived temperatures compared to actual temperatures.\n",
    "\n",
    "3. **Temp Max (Maximum Temperature)**:\n",
    "   - The maximum temperatures have a median of around **30°C**, with the IQR spanning from **27°C to 35°C**.\n",
    "   - The whiskers range from **15°C to 40°C**, showing that maximum temperatures can be quite variable.\n",
    "   - The distribution is relatively symmetrical with no outliers.\n",
    "\n",
    "4. **Feels-like Max (Maximum Perceived Temperature)**:\n",
    "   - The maximum perceived temperature has a median of around **35°C**, with a much larger IQR compared to actual maximum temperatures, spanning from **30°C to 45°C**.\n",
    "   - The whiskers extend from **20°C to over **50°C**, showing a wide range of perceived maximum temperatures.\n",
    "   - This indicates that perceived temperatures can be significantly higher than actual maximum temperatures, especially in extreme conditions.\n",
    "\n",
    "5. **Temp Min (Minimum Temperature)**:\n",
    "   - The minimum temperature has a median of **20°C**, with the IQR spanning from **18°C to 22°C**.\n",
    "   - The whiskers extend from **10°C to 30°C**, indicating the range of minimum temperatures observed.\n",
    "   - No significant outliers are present in this category.\n",
    "\n",
    "6. **Feels-like Min (Minimum Perceived Temperature)**:\n",
    "   - The minimum perceived temperature has a similar median to the actual minimum temperature, around **20°C**.\n",
    "   - However, the IQR is wider, from **18°C to 25°C**, and the whiskers show a range from **10°C to 35°C**.\n",
    "   - There are several outliers on the lower end, suggesting that there were a few instances where the perceived minimum temperature was notably lower than the majority of the observations.\n",
    "\n",
    "#### Insights:\n",
    "\n",
    "- **Temperature vs. Feels-like**: In general, the **feels-like** temperature tends to have a broader range than the actual temperature, particularly for the **maximum** temperatures. This suggests that factors like humidity, wind, or other environmental factors play a significant role in how temperatures are perceived, especially in extreme conditions.\n",
    "  \n",
    "- **Maximum Temperatures**: The **feels-like max** temperatures show a much wider distribution than actual **temp max**, suggesting that perceived heat is significantly affected by factors beyond just the recorded temperature, often making conditions feel hotter.\n",
    "\n",
    "- **Outliers in Feels-like Min**: The outliers in the **feels-like min** category indicate that in some cases, the perceived temperature is much lower than the actual temperature, likely due to environmental conditions such as wind chill or high humidity.\n",
    "\n",
    "- **Range and Variability**: Overall, the box plots indicate a **broader variability** in perceived temperatures compared to actual recorded temperatures, particularly for maximum temperatures, highlighting how environmental factors can influence human perception of temperature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms_by_year()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms illustrate the distribution of temperatures for the years **2020**, **2021**, **2022**, and **2023**. The x-axis represents the temperature in degrees Celsius (°C), and the y-axis represents the **frequency** of observations within specific temperature ranges. Each year shows how frequently different temperature values were recorded.\n",
    "\n",
    "#### 1. Temperature Distribution in 2020:\n",
    "- The temperature distribution in 2020 is relatively **even**, with no clear dominating temperature range.\n",
    "- The temperatures range from about **15°C to 32°C**, with the most frequent values falling between **25°C and 30°C**.\n",
    "- The frequency gradually increases as temperature rises, peaking in the **25°C to 30°C** range, followed by a slight decrease as temperatures reach **30°C and above**.\n",
    "\n",
    "#### 2. Temperature Distribution in 2021:\n",
    "- In 2021, the distribution shows a strong **peak** around **30°C**, with a noticeable concentration of temperature observations in this range.\n",
    "- The distribution is skewed toward higher temperatures, with fewer occurrences of lower temperatures below **15°C**.\n",
    "- There is a clear rise in frequency from **20°C to 30°C**, suggesting that **warmer temperatures** were more common in 2021, particularly around the 30°C mark.\n",
    "\n",
    "#### 3. Temperature Distribution in 2022:\n",
    "- The 2022 histogram displays a more **uniform distribution** compared to the other years, with temperatures ranging between **10°C and 32°C**.\n",
    "- The most frequent temperatures are between **25°C and 30°C**, with a gradual rise in frequency as temperatures increase from **10°C to 25°C**.\n",
    "- The distribution is less skewed than in 2021, indicating a wider range of temperatures were observed throughout the year.\n",
    "\n",
    "#### 4. Temperature Distribution in 2023:\n",
    "- Similar to 2021, the distribution in 2023 shows a **higher concentration** of temperatures in the **25°C to 30°C** range.\n",
    "- There are fewer occurrences of temperatures below **15°C**, with most temperatures observed between **20°C and 30°C**.\n",
    "- The histogram shows a slight peak around **30°C**, suggesting that **higher temperatures** were common during 2023.\n",
    "\n",
    "### Key Insights:\n",
    "- **Common Temperature Range**: Across all four years, the most frequent temperatures generally fall within the **25°C to 30°C** range, with 2021 and 2023 showing a strong peak around **30°C**.\n",
    "- **Skewed Distribution**: The temperature distributions for 2021 and 2023 are more skewed toward **higher temperatures**, indicating that warmer temperatures were more frequent in these years.\n",
    "- **Year-to-Year Variation**: While 2022 shows a relatively even distribution of temperatures, 2021 and 2023 display more pronounced peaks in the **30°C range**, suggesting that these years experienced **warmer conditions** compared to 2020 and 2022.\n",
    "- **Fewer Cold Days**: Temperatures below **15°C** are less frequent across all years, particularly in 2021 and 2023, indicating a trend toward **warmer conditions**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: PREPARE THE DATA**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 Find features(based on MI) that related to the target label:**\n",
    "\n",
    "**Why Use Mutual Information Over Correlation?**\n",
    "\n",
    "Mutual Information (MI) offers several advantages over traditional correlation measures in data analysis. Unlike correlation, which primarily captures linear relationships, MI can detect non-linear associations between variables, making it particularly valuable in complex systems such as weather prediction. MI doesn't make assumptions about data distribution, contrasting with some correlation measures that presume normal distribution. It's versatile enough to handle both continuous and categorical variables, whereas correlation typically works best with continuous data. MI's sensitivity to complex dependencies allows it to uncover intricate relationships that correlation might overlook. Additionally, MI is scale-invariant, meaning it's not affected by scale transformations of the variables. These characteristics make MI a powerful tool for feature selection and understanding variable relationships, especially in datasets where traditional correlation analysis might fall short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the MI scores of our script after compilation:**\n",
    "\n",
    "The MI score quantifies the amount of information obtained about the target variable (in this case, temperature) by observing the feature variable. Higher scores indicate stronger relationships. Here's what we can interpret from your results:\n",
    "\n",
    "- 'feelslike' has the highest MI score (3.176038), suggesting it's the most informative feature for predicting temperature. This makes sense intuitively, as \"feels like\" temperature is closely related to actual temperature.\n",
    "- 'feelslikemax' and 'tempmax' have very similar scores (1.596926 and 1.595622), indicating they provide similar amounts of information about the target.\n",
    "- 'tempmin' and 'feelslikemin' also have high scores, further emphasizing the importance of temperature-related features.\n",
    "- 'dew' point has a relatively high score (1.020049), which is reasonable as dew point is related to temperature and humidity.\n",
    "- 'sealevelpressure' has a moderate score (0.692677), suggesting some relationship with temperature.\n",
    "- Features like 'solarradiation', 'humidity', 'solarenergy', and 'cloudcover' have lower but non-negligible scores, indicating they provide some information about temperature.\n",
    "- 'snow' and 'snowdepth' have scores of 0, suggesting they provide no information about temperature in this dataset. This could be due to the absence of snow in the data or a very weak relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'dataset/Data.csv'  # Adjust the path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Select only the numeric columns from the dataset\n",
    "numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "data_numeric = data[numeric_cols]\n",
    "\n",
    "# Drop rows with missing values (NaN) in the numeric dataset\n",
    "data_numeric_cleaned = data_numeric.dropna()\n",
    "\n",
    "# Prepare the feature set (all numeric features except `temp`)\n",
    "X = data_numeric_cleaned.drop(columns=['temp', 'severerisk', 'precipprob'])  # Drop 'temp' (target variable)\n",
    "y = data_numeric_cleaned['temp']  # Target variable\n",
    "\n",
    "# Step 1: Calculate Mutual Information\n",
    "mi = mutual_info_regression(X, y)\n",
    "\n",
    "# Create a DataFrame for mutual information\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Mutual Information': mi\n",
    "}).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# Print mutual information\n",
    "print(\"Mutual Information between Features and Target:\")\n",
    "print(mi_df)\n",
    "\n",
    "# Step 2: Select top features based on MI (lấy tất cả những chỉ số MI trên 0.25)\n",
    "top_features = mi_df['Feature'].head(8).values ## Có 10 chỉ số MI trên 0.25\n",
    "print(f\"Top features correlate to feature 'temp' based on MI: {top_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1. Drop columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to keep\n",
    "features_to_keep = ['feelslike', 'tempmax', 'feelslikemax', 'tempmin', 'feelslikemin', 'dew',\n",
    "                     'sealevelpressure', 'humidity', 'temp']\n",
    "\n",
    "# Drop columns that are not in the list of features to keep\n",
    "raw_data = raw_data[features_to_keep]\n",
    "print(\"Dropped columns successfully. Feature to keeps: \", features_to_keep)\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2. Shift label for future prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to predict the temperature based on historical data. Hence we need to shift the label up so that the predicted temp is based on past data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Purpose: To predict the temperature for a future day, historical data from previous days must be used. Shifting the labels creates a relationship between past data and future predictions.\n",
    "- How It Works:\n",
    "k = -1 indicates that the \"temp\" label will be shifted up by 2 rows. This means that the current temperature value will be assigned to the label for two days prior.\n",
    "As a result, if the original data row has a temperature of 25°C on day 1, after shifting, the data row for day 1 will have a temperature label of 25°C for day 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "raw_data['temp'] = raw_data['temp'].shift(k)\n",
    "\n",
    "# Drop the rows with NaN values that were created by the shift\n",
    "raw_data = raw_data.dropna()\n",
    "\n",
    "# Optional: print the first few rows to verify\n",
    "print(raw_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3. Split-training the dataset:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our regression problem, Stratified Sampling is not necessary because the features and target values are continuous rather than categorical. Stratified Sampling is typically used in classification problems to maintain the distribution of target classes across datasets. In regression, where the goal is to predict continuous values, maintaining class proportions is irrelevant and does not impact the model's performance. Therefore, I will focus on data normalization and model optimization to enhance prediction accuracy. \n",
    "\n",
    "We split the data 80-20 (20 for test data). **The setting of random_state is to get the same training set each time, otherwise, when repeat# Set random_state to get the same training set each time, ing training many times, the modes might see all the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_set, test_set = train_test_split(raw_data, test_size=0.2, random_state=42)  \n",
    "\n",
    "# Print out some information of the split of the training set and the test set\n",
    "print('\\n____________ Split training and test set ____________')     \n",
    "print(len(train_set), \"training +\", len(test_set), \"test examples\")\n",
    "print(train_set.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 Separate labels from data, since we do not process label values (already processed)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate labels from data\n",
    "X_train = train_set.drop(columns=['temp'])\n",
    "y_train = train_set['temp']\n",
    "X_test = test_set.drop(columns=['temp'])\n",
    "y_test = test_set['temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4. Define pipelines for processing data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.1. Define ColumnSelector: a transformer for choosing columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    # Constructor takes a list of column names to select\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names  # Store the list of column names\n",
    "\n",
    "    # The fit method doesn't need to do anything, it just returns self\n",
    "    # to be compatible with scikit-learn's pipeline process\n",
    "    def fit(self, dataframe, labels=None):\n",
    "        return self\n",
    "\n",
    "    # The transform method selects columns from the DataFrame based on the list of column names\n",
    "    # and returns the values as a NumPy array\n",
    "    def transform(self, dataframe):\n",
    "        return dataframe[self.feature_names].values  # Select and return columns as a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical features\n",
    "num_feat_names = ['feelslike', 'tempmax', 'feelslikemax', 'tempmin', 'feelslikemin', 'dew', 'sealevelpressure', 'humidity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.2. Pipeline for categorical features:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, the categorical features are dropped after the MI score correlation examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.3. Pipeline for numerical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for numerical features\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', ColumnSelector(num_feat_names)),  # Select numeric columns\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\")),  # Fill missing values with median\n",
    "    ('std_scaler', StandardScaler(with_mean=True, with_std=True))  # Normalize to zero mean and unit variance\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use standardScaler?**\n",
    "\n",
    "- Data Normalization: It standardizes features to have a mean of 0 and a standard deviation of 1, ensuring all features are on the same scale.\n",
    "\n",
    "- Improved Model Performance: Many machine learning algorithms, especially distance-based ones like KNN, perform better with normalized data, leading to faster convergence and higher accuracy.\n",
    "\n",
    "- Reduced Bias: It minimizes bias towards features with larger values by bringing all features to a similar range.\n",
    "\n",
    "- Increased Stability: Standardization enhances model stability, particularly in the presence of outliers.\n",
    "\n",
    "In **summary**, StandardScaler improves model performance, accuracy, and stability.\n",
    "\n",
    "**Why impute missing data with median values?**\n",
    "\n",
    "- **Distribution preservation:** The median is less affected by outliers than the mean, helping to maintain the overall distribution of the data.\n",
    "\n",
    "- **Robustness:** The median provides a stable estimate of the central tendency of the data, particularly useful for variables with skewed distributions.\n",
    "\n",
    "- **Simplicity and efficiency:** It's an easy-to-implement and computationally efficient method, suitable for large datasets.\n",
    "\n",
    "- **Outlier handling:** The median is less sensitive to extreme values, minimizing the impact of outliers on the imputation process.\n",
    "\n",
    "- **Relationship preservation:** This method helps maintain relationships between variables better than completely removing rows with missing values.\n",
    "\n",
    "By using median imputation, you ensure that the filled-in values reflect the central tendency of the data, allowing your machine learning model to work more effectively with a complete and representative dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.4. Run the pipeline to process training data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_set_val = num_pipeline.fit_transform(train_set)\n",
    "\n",
    "# Fit the pipeline on training data and transform both training and test data\n",
    "X_train = num_pipeline.fit_transform(X_train)\n",
    "X_test = num_pipeline.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns=num_feat_names)\n",
    "X_test = pd.DataFrame(X_test, columns=num_feat_names)\n",
    "\n",
    "\n",
    "print('\\n____________ Processed feature values ____________')\n",
    "print(processed_train_set_val[:3, :]) # Print out some of the first rows of the training dataset after fit_transforming\n",
    "print(processed_train_set_val.shape)  # Print out the statistics of the training set\n",
    "joblib.dump(num_pipeline, r'models/num_pipeline.pkl')   #  Save the pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 5. TRAIN AND EVALUATE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to calculate R2 score and Root Mean Squared Error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2score_and_rmse(model, train_data, labels): \n",
    "    r2score = model.score(train_data, labels)\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    prediction = model.predict(train_data)\n",
    "    mse = mean_squared_error(labels, prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return r2score, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Store and Load Models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_model(model, model_name = \"\"):\n",
    "    # NOTE: sklearn.joblib faster than pickle of Python\n",
    "    # INFO: can store only ONE object in a file\n",
    "    if model_name == \"\": \n",
    "        model_name = type(model).__name__\n",
    "    joblib.dump(model,'models/' + model_name + '_model.pkl')\n",
    "    print(f\"Model successfully saved as \" + model_name + '_model.pkl')\n",
    "    \n",
    "def load_model(model_name):\n",
    "    # Load objects into memory\n",
    "    #del model\n",
    "    model = joblib.load('models/' + model_name + '_model.pkl')\n",
    "    #print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1. Try Light GBM model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor() #fix here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________ LGBMRegressor ____________')\n",
    "\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "\n",
    "# Predict labels for some test instances\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "store_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2. Try XGBoost model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor() #fix here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________ XGBoost_Regressor ____________')\n",
    "\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "\n",
    "# Predict labels for some test instances\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "store_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3. Try Decision Tree (in-lecture).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor() #fix here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________DecisionTreeRegressor____________')\n",
    "\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "# \n",
    "\n",
    "# Predict labels for some test instances\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "store_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.4. Try Polynomial Regression (in-lecture).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng PolynomialFeatures kết hợp với một mô hình hồi quy\n",
    "degree = 2  # Độ của đa thức bạn muốn tạo\n",
    "model = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=degree)),\n",
    "    ('lin_reg', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________PolynomialRegressor____________')\n",
    "\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "# Predict labels for some test instances\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]).round(decimals=1))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "store_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.5. Try Linear Regressor (in-lecture).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #fix here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________LinearRegressor____________')\n",
    "\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "# \n",
    "\n",
    "# Predict labels for some test instances\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]).round(decimals=1))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "store_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.6. Try Random Forest (in-lecture).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators = 5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________RandomForestRegressor____________')\n",
    "\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "\n",
    "\n",
    "# Predict labels for some test instances\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]).round(decimals=1))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "store_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.7. Try K-Nearest-Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(n_neighbors=5)  # fix here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________KNeighborsRegressor____________')\n",
    "\n",
    "# Tính toán r2 score và rmse\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "# Dự đoán nhãn cho một số mẫu thử nghiệm\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]).round(decimals=1))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "# Lưu mô hình đã huấn luyện\n",
    "store_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.8. Try using SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='rbf')  # fix here, với kernel 'rbf' mặc định\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________Support Vector Regressor (SVR)____________')\n",
    "\n",
    "# Tính toán r2 score và rmse\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "# Dự đoán nhãn cho một số mẫu thử nghiệm\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]).round(decimals=1))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "# Lưu mô hình đã huấn luyện\n",
    "store_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.9. Evaluate with K-fold cross validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this K-Cross Validation, we will evaluate the models based on their RMSE scores statistics and their residual distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why We Chose to Evaluate the Model Using Average RMSE\n",
    "\n",
    "In our temperature prediction project, we have chosen to evaluate model performance using the **Average RMSE (Root Mean Squared Error)** after applying k-fold cross-validation. This decision is based on several key reasons:\n",
    "\n",
    "1. **Measuring Absolute Errors**: RMSE allows us to measure the average absolute error between the model’s predictions and the actual temperature values. This is crucial for our project, as it requires a model that can provide the most accurate predictions possible. A low RMSE ensures that the prediction errors are minimal, reducing the risk when applying the model in real-world scenarios.\n",
    "\n",
    "2. **Weighting Larger Errors**: RMSE emphasizes larger errors (outliers) by squaring the differences, which helps us identify and optimize the model to minimize the most inaccurate predictions. This is particularly important in temperature prediction, where large errors can lead to inaccurate decisions in control systems or forecasting.\n",
    "\n",
    "3. **Intuitive Interpretation**: RMSE is calculated in the same units as the target variable (temperature), making it easier to interpret the magnitude of the model's errors. For instance, if a model’s RMSE is 1.5°C, we can easily visualize that the model’s predictions deviate from the actual values by an average of 1.5°C, making it easier to communicate this information to stakeholders.\n",
    "\n",
    "4. **Avoiding Assumptions About Variance**: The \\(R^2\\) metric measures how much of the variance in the data is explained by the model, but in many cases, \\(R^2\\) may not provide enough information about the actual prediction errors. Using RMSE allows us to focus on real accuracy rather than just measuring how well the model fits the data.\n",
    "\n",
    "Therefore, choosing **Average RMSE** ensures that our model not only fits the data but also provides reliable predictions with high accuracy. This aligns with the core objectives of the project and maximizes the value our product delivers to the end users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why use the Residuals Distribution Plot\n",
    "\n",
    "We also used the **Residual Distribution Plot** to evaluate a model's performance due to these reasons:\n",
    "\n",
    "1. **Assessing Model Assumptions and Fit**: By using this plot, we can verify if residuals are normally distributed around zero. Based on that, we can detect bias suggesting that the model may consistently over- or under-predicting, or identify outliers.\n",
    "\n",
    "2. **Complementing Numerical Metrics**: Using the plot help us gained insights not captured by Numerical Metrics (Avg. RMSE in this case). For example, a model can have a low RMSE yet have outliers that are way to extreme may not be a suitable model. The plot can reveal this and help us to compare different models performance.\n",
    "\n",
    "Hence, usinng the **Residual Distribution Plot** help us to have a visual and intuitive way assess a model performance. By complementing this plot with numerical metrics, our team can have a more firm reasoning and decision to choose the most suitable for the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv1 = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42); \n",
    "# cv2 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42); \n",
    "# cv3 = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42); \n",
    "\n",
    "# Function to calculate R² score and RMSE\n",
    "# def r2score_and_rmse(model, X, y):\n",
    "#     y_pred = model.predict(X)\n",
    "#     r2 = r2_score(y, y_pred)\n",
    "#     rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "#     return r2, rmse\n",
    "\n",
    "\n",
    "print('\\n____________ K-fold cross validation ____________')\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=37) # cv data generator\n",
    "\n",
    "run_new_evaluation = 1\n",
    "if run_new_evaluation:\n",
    "    #Evaluate LinearRegression\n",
    "    model_name = \"LinearRegression\"\n",
    "    model = LinearRegression()\n",
    "    nmse_scores = cross_val_score(model, X_train, y_train,cv=cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-nmse_scores)\n",
    "    joblib.dump(rmse_scores,'saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"LinearRegression rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    # Step 2: Predict using cross-validation\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "    # Step 3: Calculate residuals\n",
    "    residuals = y_train - y_train_pred\n",
    "    # Step 4: Plot the residual distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Linear Regression Residual Distribution')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    model = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(degree=degree)),\n",
    "        ('lin_reg', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation and evaluate\n",
    "    nmse_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-nmse_scores)\n",
    "\n",
    "    # Save the RMSE scores\n",
    "    model_name = \"PolynomialRegression\"\n",
    "    joblib.dump(rmse_scores, 'saved_objects/' + model_name + '_rmse.pkl')\n",
    "\n",
    "    print(\"Polynomial regression RMSE: \", rmse_scores.round(decimals=1))\n",
    "    print(\"Avg. RMSE: \", np.mean(rmse_scores), '\\n')\n",
    "\n",
    "    # Step 2: Perform cross-validation predictions\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "\n",
    "    # Step 3: Calculate residuals\n",
    "    residuals = y_train - y_train_pred\n",
    "\n",
    "    # Step 4: Plot the residual distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Residual Distribution for Polynomial Regression')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    model_name = \"DecisionTreeRegressor\" \n",
    "    model = DecisionTreeRegressor()\n",
    "    nmse_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-nmse_scores)\n",
    "    joblib.dump(rmse_scores,'saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"DecisionTreeRegressor rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "\n",
    "    # Step 2: Perform cross-validation predictions\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "\n",
    "    # Step 3: Calculate residuals\n",
    "    residuals = y_train - y_train_pred\n",
    "\n",
    "    # Step 4: Plot the residual distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Residual Distribution for Decision Tree Regression')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate RandomForestRegressor\n",
    "    model_name = \"RandomForestRegressor\" \n",
    "    model = RandomForestRegressor(n_estimators = 5)\n",
    "    nmse_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-nmse_scores)\n",
    "    joblib.dump(rmse_scores,'saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"RandomForestRegressor rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    # Step 2: Perform cross-validation predictions\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "\n",
    "    # Step 3: Calculate residuals\n",
    "    residuals = y_train - y_train_pred\n",
    "\n",
    "    # Step 4: Plot the residual distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Residual Distribution for Random Forest Regression')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    #Evaluate LGBM\n",
    "    model_name = \"LightGBM\"\n",
    "    model = lgb.LGBMRegressor(verbose = -1)\n",
    "    nmse_scores = cross_val_score(model, X_train, y_train, cv = cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-nmse_scores)\n",
    "    joblib.dump(rmse_scores, 'saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"LightGBM rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    # Step 2: Perform cross-validation predictions\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "\n",
    "    # Step 3: Calculate residuals\n",
    "    residuals = y_train - y_train_pred\n",
    "\n",
    "    # Step 4: Plot the residual distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Residual Distribution for Light GBM')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    #Evaluate XGB BoostRegressor\n",
    "    model_name = \"XGBoostRegressor\"\n",
    "    model = XGBRegressor()\n",
    "    nmse_scores = cross_val_score(model, X_train, y_train, cv = cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-nmse_scores)\n",
    "    joblib.dump(rmse_scores, 'saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"XGBoost rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    # Step 2: Perform cross-validation predictions\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "\n",
    "    # Step 3: Calculate residuals\n",
    "    residuals = y_train - y_train_pred\n",
    "\n",
    "    # Step 4: Plot the residual distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Residual Distribution for XGB')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    #Evaluate K nearest Neighbor\n",
    "    model_name = \"KNeighbor\"\n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    nmse_scores = cross_val_score(model, X_train, y_train, cv = cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-nmse_scores)\n",
    "    joblib.dump(rmse_scores, 'saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"KNeighbor rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    # Step 2: Perform cross-validation predictions\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "\n",
    "    # Step 3: Calculate residuals\n",
    "    residuals = y_train - y_train_pred\n",
    "\n",
    "    # Step 4: Plot the residual distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Residual Distribution for KNN')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    #Evaluate SVR M\n",
    "    model_name = \"SVR_model\"\n",
    "    model = SVR(kernel='rbf')\n",
    "    nmse_scores = cross_val_score(model, X_train, y_train, cv = cv, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-nmse_scores)\n",
    "    joblib.dump(rmse_scores, 'saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"SVR rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    # Step 2: Perform cross-validation predictions\n",
    "    y_train_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "\n",
    "    # Step 3: Calculate residuals\n",
    "    residuals = y_train - y_train_pred\n",
    "\n",
    "    # Step 4: Plot the residual distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Residual Distribution for SVR')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    #Load LinearRegression\n",
    "    model_name = \"LinearRegression\" \n",
    "    rmse_scores = joblib.load('saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"\\nLinearRegression rmse: \", rmse_scores.round(decimals=1))\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "    \n",
    "    #Load PolynomialRegression:\n",
    "    model_name = \"PolinomialRegression\" \n",
    "    rmse_scores = joblib.load('saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"Polinomial regression rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "    \n",
    "    #Load DecisionTreeRegressor\n",
    "    model_name = \"DecisionTreeRegressor\" \n",
    "    rmse_scores = joblib.load('saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"DecisionTreeRegressor rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "    \n",
    "    #Load RandomForestRegressor\n",
    "    model_name = \"RandomForestRegressor\" \n",
    "    rmse_scores = joblib.load('saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"RandomForestRegressor rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    #Load LightGBM\n",
    "    model_name = \"LightGBM\"\n",
    "    rmse_scores = joblib.load('saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"LightGBM rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    #Load XGB\n",
    "    model_name = \"XGBoostRegressor\"\n",
    "    rmse_scores = joblib.load('saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"XGBoost rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    #Load KNeighbor\"\n",
    "    model_name = \"KNeighbor\"\n",
    "    rmse_scores = joblib.load('saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"KNeighbor rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')\n",
    "\n",
    "    #Load \"SVR_model\"\n",
    "    model_name = \"SVR_model\"\n",
    "    rmse_scores = joblib.load('saved_objects/' + model_name + '_rmse.pkl')\n",
    "    print(\"SVR rmse: \", rmse_scores)\n",
    "    print(\"Avg. rmse: \", mean(rmse_scores),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.10 So which one to choose?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our team based on the average RMSE and the residual distribution plot to determine the best model. Below is our observations based on the average RMSE and te the residual plot distributions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear Regression\n",
    "    * Avg. RMSE: 1.7021388875207208. This is a good RMSE. However, there are more factors to consider.\n",
    "    * There is some skewness to the left in the residual distribution. This model also has some “extreme” points when the model produces the result nearly 10 degrees different from the actual value. This suggests that the model may not be accurate and suitable.\n",
    "* Polynomial Regression\n",
    "    * Avg. RMSE: 1.6929377548064592. This is a relatively low RMSE. However, there are more factors to consider.\n",
    "    * The residual distribution plot does show a left skew. This model also has some “extreme” points when the model produces the result nearly 10 degrees different from the actual value. This suggests that the model may not be accurate and suitable.\n",
    "* Decision Tree Regression\n",
    "    * Avg. RMSE: 2.3814286454896973. This model has a higher RMSE when compared to other models. This may imply the model’s performance might not be as good as other models.\n",
    "    * The residual distribution for the decision tree regression appears more symmetric and centered around zero compared to the polynomial regression. However, as mentioned before, the model has a higher RMSE score compared to other models, suggesting that this model might not be suitable. The distribution range is also wider with some days the model may return the result -10 degree difference from the actual value. This suggests that the model may not be accurate and suitable.\n",
    "* Random Forest Regression\n",
    "    * Avg. RMSE: 1.9042304543237498. This is a low RMSE, although being slightly higher than Linear Regression and Polynomial Regression.\n",
    "    * The distribution is quite symmetric around zero and more tightly clustered around zero compared to previous models, suggesting better accuracy. Howver, the range of distribution is quite high when compared to other models.\n",
    "* LightGBM\n",
    "    * Avg. RMSE: 1.7756602572875178. This is a low RMSE.\n",
    "    * The distribution is quite symmetric around zero. However, it can be seen that there are some outliers on the left side. This model is slightly wider spread compared to Random Forest Regression. Howeveer, it has a smaller range of distribution when compared to other models.\n",
    "* XGBoost\n",
    "    * Avg. RMSE: 1.8549025611541374. This is a low RMSE, although slightly higher than some other models.\n",
    "    * The distribution is quite the same as LightGBM, quite symmetric around zero and some outliers value on the left side. The spread is also higher than Random Forest Regression. However, the range of the distribution is higher when compared to some other models such as LightGBM.\n",
    "* K-Nearest Neighbor\n",
    "    * Avg. RMSE: 1.7831833116230849. This is a low RMSE, althoght slightly higher than some other models. Let's consider other factor.\n",
    "    * The distribution is quite symmetrical, however its spread is slightly wider compared to some other models, suggesting lower accuracy.\n",
    "* SVR\n",
    "    * Avg. RMSE: 1.7558505444990793. This is a low RMSE, but let's also consider other factors.\n",
    "    * The distribution is skewed to the left. Although the distribution is tightly clustered around zero, however, there are some “extreme” points where the model may predict wrong up to -10 degree, suggesting this model may not be accurate.\n",
    "\n",
    "\n",
    "After evaluation, our team decided to choose the LightGBM model for fine-tuning due to its relatively low RMSE. Although the model might not have the lowest RMSE score, the residuals distribution plot suggests that this model might be suitable with its distribution centered around zero, smaller range of distribution and not too many “extreme” outliers where the model produces results far different from the actual value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By evaluation on the residual distribution plot and the average rmse score, our team decided to fine tune the LightGBM model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 6: FINE TUNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As disscussed, our team decided to choose the LightGBM to fine-tune.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________ Fine-tune models ____________\n",
      "\n",
      "====== Fine-tune LGBMRegressor ======\n",
      "Best hyperparameter combination:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "Best rmse:  1.7208380610794025\n",
      "Performance of hyperparameter combinations:\n",
      "rmse = 2.534336093206423 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.534336093206423 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.5364055145867193 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.5364055145867193 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.5352699628623703 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.5352699628623703 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.5365552226380834 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.5365552226380834 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8640338900099453 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8640338900099453 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8639616168446216 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8639616168446216 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8641982759921414 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8641982759921414 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8639707156018002 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8639707156018002 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.49197803959312 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.49197803959312 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.495352266936643 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.495352266936643 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.4931695020327904 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.4931695020327904 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.49716694302193 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.49716694302193 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8486515244940602 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8486515244940602 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8496145127147703 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8496145127147703 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8492268670877199 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8492268670877199 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8507879920917898 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8507879920917898 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.4872153716469505 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.4872153716469505 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.491647563415479 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.491647563415479 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.488924634768981 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.488924634768981 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.4931684379583174 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.4931684379583174 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8419673184554086 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8419673184554086 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8448898721145066 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8448898721145066 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.844545043568916 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.844545043568916 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8479735689217343 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8479735689217343 {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7289015626529836 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7289015626529836 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7240337774201846 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7240337774201846 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7237557558290189 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7237557558290189 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7208380610794025 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7208380610794025 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7345235103094658 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7345235103094658 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7308638664356235 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7308638664356235 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7298969462834541 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7298969462834541 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.737301373994306 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.737301373994306 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7435897488462135 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7435897488462135 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7456408834974622 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7456408834974622 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7497491517778292 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7497491517778292 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7510155310433901 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7510155310433901 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.765412447052736 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.765412447052736 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7603755502624852 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7603755502624852 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7648156666088768 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7648156666088768 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7662067821506036 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7662067821506036 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.75022541898321 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.75022541898321 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.757258076461478 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.757258076461478 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.756524343196314 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.756524343196314 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7575032734660596 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7575032734660596 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7756055212206894 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7756055212206894 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7792004718230263 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7792004718230263 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7777378316246395 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7777378316246395 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7817919323947886 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7817919323947886 {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.5277419926755287 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.5277419926755287 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.529674579521754 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.529674579521754 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.528570818445754 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.528570818445754 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.52989609358619 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.52989609358619 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8646521480964484 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8646521480964484 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8654158911699747 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8654158911699747 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8655489543398225 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8655489543398225 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8656351364932455 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8656351364932455 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.4848693405751447 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.4848693405751447 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.487622737569257 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.487622737569257 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.485870030233954 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.485870030233954 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.4886695445636304 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.4886695445636304 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8399595908979476 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8399595908979476 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8411830565068155 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8411830565068155 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8399182252528192 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8399182252528192 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8413642652166442 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8413642652166442 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.483094066251522 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.483094066251522 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.486067056974005 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.486067056974005 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 2.483678870177303 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 2.483678870177303 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 2.488146370450709 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 2.488146370450709 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8435791258351637 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8435791258351637 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.8443141970080439 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.8443141970080439 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.8426336383805029 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.8426336383805029 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.845925812790744 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.845925812790744 {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7356222206916954 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7356222206916954 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7367078290847597 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7367078290847597 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7366774392281745 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7366774392281745 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7335009147953346 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7335009147953346 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7505502214814925 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7505502214814925 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.746197796720138 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.746197796720138 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7426976439572124 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7426976439572124 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7421962137316103 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7421962137316103 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.748963980349695 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.748963980349695 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7433243099269764 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7433243099269764 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7553669663072737 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7553669663072737 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7435987246099494 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7435987246099494 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7682951893102927 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7682951893102927 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7622573932386485 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7622573932386485 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7732671074287798 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7732671074287798 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.764819201227021 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.764819201227021 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7605697511295542 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7605697511295542 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7618040838355835 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7618040838355835 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7641334398413795 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7641334398413795 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7587251241434134 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7587251241434134 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7850178902065734 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7850178902065734 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7822326524433443 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7822326524433443 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0.1, 'subsample': 1.0}\n",
      "rmse = 1.7866747151952798 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "rmse = 1.7866747151952798 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "rmse = 1.7716542516381266 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.8}\n",
      "rmse = 1.7716542516381266 {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "\n",
    "print('\\n____________ Fine-tune models ____________')\n",
    "\n",
    "# Function to print results of grid search\n",
    "def print_search_result(grid_search, model_name=\"\"):\n",
    "    print(\"\\n====== Fine-tune \" + model_name +\" ======\")\n",
    "    print('Best hyperparameter combination: ',grid_search.best_params_)\n",
    "    print('Best rmse: ', np.sqrt(-grid_search.best_score_)) \n",
    "    print('Performance of hyperparameter combinations:')\n",
    "    cv_results = grid_search.cv_results_\n",
    "    for (mean_score, params) in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n",
    "        print('rmse =', np.sqrt(-mean_score), params)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=37)\n",
    "\n",
    "run_new_search = 0\n",
    "if run_new_search:\n",
    "    # Define a parameter grid for LightGBM\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1],\n",
    "        'reg_lambda': [0, 0.1]\n",
    "    }\n",
    "    \n",
    "    # Set up and run grid search\n",
    "    lgb_model = LGBMRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Save grid search results\n",
    "    joblib.dump(grid_search, 'saved_objects/LGBMRegressor_gridsearch.pkl')\n",
    "\n",
    "    # Print search results\n",
    "    print_search_result(grid_search, \"LGBMRegressor\")\n",
    "\n",
    "    # Get best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "else:\n",
    "    # Load previously saved grid search results\n",
    "    grid_search = joblib.load('saved_objects/LGBMRegressor_gridsearch.pkl')\n",
    "    print_search_result(grid_search, model_name=\"LGBMRegressor\")\n",
    "    \n",
    "    # Get best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions on training data\n",
    "    y_train_pred = best_model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STEP 7: ANALYZE AND TEST YOUR SOLUTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.1. Pick the best model - the SOLUTION:**\n",
    "As we have explained above, we have choose and fine-tuned XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________ ANALYZE AND TEST SOLUTION ____________\n",
      "SOLUTION:  LGBMRegressor(colsample_bytree=0.8, max_depth=3, random_state=42, reg_alpha=0.1,\n",
      "              reg_lambda=0.1, subsample=0.8)\n",
      "Model successfully saved as SOLUTION_model.pkl\n"
     ]
    }
   ],
   "source": [
    "search = joblib.load('saved_objects/LGBMRegressor_gridsearch.pkl')\n",
    "best_model = search.best_estimator_\n",
    "print('\\n____________ ANALYZE AND TEST SOLUTION ____________')\n",
    "print('SOLUTION: ' , best_model)\n",
    "store_model(best_model, model_name=\"SOLUTION\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.2 Analyze the SOLUTION to get more insights about the data**\n",
    "(NOTE: For LightGBM Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features and importance scores:\n",
      "humidity: 101.0000\n",
      "feelslike: 99.0000\n",
      "dew: 85.0000\n",
      "sealevelpressure: 72.0000\n",
      "tempmax: 71.0000\n",
      "feelslikemax: 49.0000\n",
      "tempmin: 46.0000\n",
      "feelslikemin: 36.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if type(best_model).__name__ == \"LGBMRegressor\":\n",
    "    # Get feature importances\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    \n",
    "    # Get feature names (assuming X_train.columns contains your feature names)\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    # Create a list of tuples with feature names and their importance scores\n",
    "    feature_importance_pairs = list(zip(feature_names, feature_importances))\n",
    "    \n",
    "    # Sort the pairs by importance score in descending order\n",
    "    sorted_importances = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print('\\nFeatures and importance scores:')\n",
    "    for name, importance in sorted_importances:\n",
    "        print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "    # Optionally, you can create a bar plot of feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar([x[0] for x in sorted_importances], [x[1] for x in sorted_importances])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('LightGBM Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7.3 Run on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance on test data:\n",
      "R2 score (on test data, best=1): 0.9056101051629488\n",
      "Root Mean Square Error:  1.6368653852409232\n",
      "\n",
      "Test data: \n",
      "    feelslike   tempmax  feelslikemax   tempmin  feelslikemin       dew  \\\n",
      "0   1.531874  1.296318      1.403462  0.637695      0.376529  1.048752   \n",
      "1   1.275963  0.775198      1.215943  1.041398      1.343747  1.065685   \n",
      "2   0.213933 -0.065317      0.038739  0.637695      0.376529  0.794765   \n",
      "3   0.329093  0.943301      0.872158  0.435844      0.215326  0.540777   \n",
      "4   1.250372  1.111404      1.143019  1.041398      1.343747  0.913292   \n",
      "5  -0.988848 -1.174797     -1.138464 -0.775264     -0.751891 -0.559836   \n",
      "6  -0.758528 -0.737729     -0.867603 -0.573413     -0.590688 -0.255051   \n",
      "7   0.789732  0.775198      0.955500  0.839546      0.537732  0.947157   \n",
      "8  -1.001643 -0.670488     -0.825932 -1.421188     -1.267741 -2.083761   \n",
      "\n",
      "   sealevelpressure  humidity  \n",
      "0         -1.182361  0.053713  \n",
      "1         -1.351989  0.567749  \n",
      "2         -0.574529  1.109322  \n",
      "3         -1.139955  0.246476  \n",
      "4         -0.461444 -0.028900  \n",
      "5          0.386694  0.742154  \n",
      "6          1.178290  0.760512  \n",
      "7         -0.885513  0.751333  \n",
      "8          1.248968 -2.553184  \n",
      "\n",
      "Predictions:  [29.8 29.  27.3 28.1 30.1 20.2 21.4 29.1 18.6]\n",
      "Labels:       [28.4, 28.7, 25.7, 27.2, 26.9, 21.8, 20.5, 29.0, 18.5] \n",
      "\n",
      "\n",
      "Processed: \n",
      " [[ 1.53187382  1.29631765  1.40346229 ...  1.04875243 -1.18236145\n",
      "   0.05371263]\n",
      " [ 1.275963    0.77519828  1.21594313 ...  1.06568493 -1.3519891\n",
      "   0.5677485 ]\n",
      " [ 0.21393312 -0.06531684  0.0387395  ...  0.79476483 -0.57452907\n",
      "   1.10932202]\n",
      " ...\n",
      " [ 0.9304834   1.02735281  0.86174026 ...  0.67623729 -0.37663016\n",
      "  -0.43278561]\n",
      " [-0.05477323  0.10278618  0.05957496 ...  0.48997972 -0.8148349\n",
      "   0.79722881]\n",
      " [-2.23001515 -2.687724   -2.07605995 ... -2.69333149  2.50703975\n",
      "  -0.98353834]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itsld/.local/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#%% 7.3 Run on test data\n",
    "try:\n",
    "    num_pipeline = joblib.load(r'models/num_pipeline.pkl')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: num_pipeline.pkl file not found. Make sure you have saved the pipeline.\")\n",
    "    # Nếu không tìm thấy file, bạn có thể cần phải tạo và lưu pipeline trước\n",
    "\n",
    "processed_test_set = num_pipeline.transform(test_set)\n",
    "# 7.3.1 Compute R2 score and root mean squared error\n",
    "r2score, rmse = r2score_and_rmse(best_model, processed_test_set, y_test)\n",
    "print('\\nPerformance on test data:')\n",
    "print('R2 score (on test data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "\n",
    "# 7.3.2 Predict labels for some test instances\n",
    "print(\"\\nTest data: \\n\", X_test.iloc[0:9])\n",
    "print(\"\\nPredictions: \", best_model.predict(processed_test_set[0:9]).round(decimals=1))\n",
    "print(\"Labels:      \", list(y_test[0:9]),'\\n')\n",
    "print(\"\\nProcessed: \\n\", processed_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overall Insight from the Predictions vs. Labels Analysis**\n",
    "\n",
    "The comparison between the predicted values and the actual labels shows a pattern of slight overestimation across most instances. While the differences between predicted and actual values are generally small, there are a few notable instances where the model significantly overestimates, indicating room for improvement.\n",
    "\n",
    "#### **Key Observations**:\n",
    "\n",
    "- **General Overestimation**: The model tends to predict values slightly higher than the actual labels. In 6 out of 9 instances, the model’s predictions are greater than the actual values. This suggests that the model may be biased towards overestimating the outcomes, which could be due to an overfitting tendency on higher values in the training data.\n",
    "\n",
    "- **Small Errors in Most Cases**: Despite the overestimation, the majority of the differences between the predicted and actual values are relatively small, typically within a range of 0.3 to 1.6. This indicates that the model is generally performing well and is capable of making fairly accurate predictions in most cases.\n",
    "\n",
    "- **Significant Overestimation in Some Cases**: In certain cases, such as Instance 5, the model's overestimation is more pronounced, with a difference of +3.2. This suggests that the model might be struggling with specific conditions or features in the data that cause it to predict higher values than expected. It could benefit from fine-tuning or retraining with additional data to improve performance on these outliers.\n",
    "\n",
    "- **Close Performance on Several Instances**: In instances like Instance 2, where the difference is only 0.3, the model demonstrates that it is capable of making very accurate predictions. This shows promise for the model’s overall robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### **Enhanced Improvement Areas for Model Performance**\n",
    "\n",
    "Building upon the initial insights from the comparison between predictions and labels, it's clear that while the model demonstrates commendable accuracy in several instances, there are specific areas where targeted improvements can significantly enhance its overall performance. Below are detailed recommendations to address the identified issues and further optimize the model:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Addressing the Overestimation Bias**\n",
    "\n",
    "**Issue Identified**: The model exhibits a tendency to slightly overestimate the actual values in the majority of instances.\n",
    "\n",
    "**Improvement Strategies**:\n",
    "\n",
    "- **Adjust the Loss Function**:\n",
    "  - **Use Asymmetric Loss Functions**: Implement loss functions that penalize overestimation more heavily than underestimation. For example, Quantile Loss can be tailored to reduce bias in predictions.\n",
    "  - **Custom Loss Functions**: Design a custom loss function that incorporates penalties for overestimation, encouraging the model to balance its predictions more effectively.\n",
    "\n",
    "- **Regularization Techniques**:\n",
    "  - **L1 and L2 Regularization**: Apply regularization to reduce model complexity and prevent overfitting, which can contribute to biased predictions.\n",
    "  - **Elastic Net**: Combine L1 and L2 regularization to leverage the benefits of both methods, promoting sparsity and reducing variance.\n",
    "\n",
    "- **Bias Correction Post-Processing**:\n",
    "  - **Calibration Techniques**: Implement calibration methods such as Platt Scaling or Isotonic Regression to adjust the model's output probabilities and reduce bias.\n",
    "  - **Scaling Predictions**: Apply a scaling factor to the predictions based on the observed bias to bring them closer to the actual values.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Enhancing Model Robustness on Outliers**\n",
    "\n",
    "**Issue Identified**: Significant overestimation in specific instances (e.g., Instance 5) indicates the model struggles with certain outliers or edge cases.\n",
    "\n",
    "**Improvement Strategies**:\n",
    "\n",
    "- **Robust Training Data**:\n",
    "  - **Augment Training Data**: Incorporate more samples similar to the outliers to help the model learn from these scenarios.\n",
    "  - **Synthetic Data Generation**: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic samples that represent underrepresented cases.\n",
    "\n",
    "- **Advanced Feature Engineering**:\n",
    "  - **Interaction Features**: Create interaction terms between features that might influence outlier behavior.\n",
    "  - **Polynomial Features**: Incorporate polynomial features to capture non-linear relationships that may be causing the model to mispredict outliers.\n",
    "\n",
    "- **Model Ensemble Techniques**:\n",
    "  - **Stacking**: Combine multiple models to leverage their strengths and mitigate individual weaknesses, improving performance on outliers.\n",
    "  - **Boosting**: Use ensemble methods like XGBoost or LightGBM that focus on correcting the errors of previous models, enhancing robustness against outliers.\n",
    "\n",
    "- **Anomaly Detection Preprocessing**:\n",
    "  - **Detect and Handle Outliers**: Implement anomaly detection algorithms (e.g., Isolation Forest, DBSCAN) to identify and appropriately handle outliers before training, either by removing them or treating them differently.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Optimizing Feature Selection and Engineering**\n",
    "\n",
    "**Issue Identified**: Potential room for improvement in the features used for model training, which can impact prediction accuracy.\n",
    "\n",
    "**Improvement Strategies**:\n",
    "\n",
    "- **Feature Importance Analysis**:\n",
    "  - **Utilize Feature Importance Metrics**: Leverage model-based feature importance (e.g., from Random Forests, XGBoost) to identify and retain the most impactful features.\n",
    "  - **Recursive Feature Elimination (RFE)**: Systematically remove less important features to enhance model performance and reduce overfitting.\n",
    "\n",
    "- **Dimensionality Reduction**:\n",
    "  - **Principal Component Analysis (PCA)**: Apply PCA to reduce feature dimensionality while preserving variance, potentially improving model efficiency and performance.\n",
    "  - **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: Use t-SNE for visualizing high-dimensional data, aiding in better feature engineering decisions.\n",
    "\n",
    "- **Feature Scaling and Transformation**:\n",
    "  - **Standardization**: Ensure all features are on a comparable scale using StandardScaler or MinMaxScaler, which can improve model convergence and performance.\n",
    "  - **Log Transformation**: Apply log transformations to skewed features to stabilize variance and make the data more normal distribution-like.\n",
    "\n",
    "- **Incorporate Domain Knowledge**:\n",
    "  - **Create Domain-Specific Features**: Utilize insights from domain experts to engineer features that capture essential patterns and relationships relevant to the prediction task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
