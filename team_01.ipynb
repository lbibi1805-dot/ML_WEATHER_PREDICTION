{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import OneHotEncoder      \n",
    "from sklearn.model_selection import KFold   \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib \n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1. LOOK AT THE BIG PICTURE (DONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2. GET THE DATA (DONE). LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r'dataset/Data.csv')\n",
    "# raw_data.head(10) preview the first 10 column\n",
    "#raw_data.tail() preview the last 10 column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3. DISCOVER THE DATA TO GAIN INSIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting data for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the data for each year\n",
    "def plot_year_data(year):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax2 = ax.twinx()  # Create a secondary axis for the bar chart\n",
    "\n",
    "    # Plot temperature and feels like as line charts\n",
    "    ax.plot(monthly_avg.index, monthly_avg['temp'][year], label=f'Temp {year}', color='red')\n",
    "    ax.plot(monthly_avg.index, monthly_avg['feelslike'][year], label=f'Feels Like {year}', color='red', linestyle='dashed')\n",
    "\n",
    "    # Plot precipitation as a bar chart\n",
    "    ax2.bar(monthly_avg.index, monthly_avg['precip'][year], color='blue', alpha=0.3, label=f'Precip {year}')\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax.set_title(f'Temperature, Feels Like, and Precipitation in {year}')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Temperature (¬∞C)', color='red')\n",
    "    ax2.set_ylabel('Precipitation (mm)', color='blue')\n",
    "\n",
    "    # Set custom month labels on the x-axis\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels(months)\n",
    "\n",
    "    # Add legends outside of the plot area, further right\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.15, 1))\n",
    "    ax2.legend(loc='upper left', bbox_to_anchor=(1.15, 0.85))\n",
    "\n",
    "    # Grid for better readability\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Adjust layout and leave space for the legends\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Quick view of the data\n",
    "print('\\n____________ Dataset info ____________')\n",
    "print(raw_data.info())              \n",
    "print('\\n____________ Some first data examples ____________')\n",
    "print(raw_data.head(5)) \n",
    "print('\\n____________ Counts on a feature ____________')\n",
    "# print(raw_data['LEGAL DOCUMENTS'].value_counts()) \n",
    "print('\\n____________ Statistics of numeric features ____________')\n",
    "print(raw_data.describe())    \n",
    "print('\\n____________ Get specific rows and cols ____________')     \n",
    "print(raw_data.iloc[[0,1,45], [2, 5]] ) # Refer using column ID\n",
    "\n",
    "\n",
    "# Convert 'datetime' column to pandas datetime\n",
    "raw_data['datetime'] = pd.to_datetime(raw_data['datetime'])\n",
    "\n",
    "# Extract the year from the 'datetime' column into a new 'year' column\n",
    "raw_data['year'] = raw_data['datetime'].dt.year\n",
    "\n",
    "# Filter data for the years 2020 to 2024\n",
    "years = [2020, 2021, 2022, 2023, 2024]\n",
    "filtered_data = raw_data[raw_data['year'].isin(years)]\n",
    "\n",
    "# Group the data by year and month, and calculate the average temp, feels like, and precip for each month\n",
    "monthly_avg = filtered_data.groupby([filtered_data['datetime'].dt.month, filtered_data['datetime'].dt.year])[['temp', 'feelslike', 'precip']].mean().unstack(1)\n",
    "\n",
    "# Define month names for the x-axis\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Plot the data for each year separately (2020 to 2023)\n",
    "for year in years[:-1]:  # Exclude 2024\n",
    "    plot_year_data(year)\n",
    "\n",
    "# Plot the data for 2024 in a separate figure\n",
    "plot_year_data(2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Find features(based on MI) that related to the target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'dataset/Data.csv'  # Adjust the path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Select only the numeric columns from the dataset\n",
    "numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "data_numeric = data[numeric_cols]\n",
    "\n",
    "# Drop rows with missing values (NaN) in the numeric dataset\n",
    "data_numeric_cleaned = data_numeric.dropna()\n",
    "\n",
    "# Prepare the feature set (all numeric features except `temp`)\n",
    "X = data_numeric_cleaned.drop(columns=['temp', 'severerisk', 'precipprob'])  # Drop 'temp' (target variable)\n",
    "y = data_numeric_cleaned['temp']  # Target variable\n",
    "\n",
    "# Step 1: Calculate Mutual Information\n",
    "mi = mutual_info_regression(X, y)\n",
    "\n",
    "# Create a DataFrame for mutual information\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Mutual Information': mi\n",
    "}).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# Print mutual information\n",
    "print(\"Mutual Information between Features and Target:\")\n",
    "print(mi_df)\n",
    "\n",
    "# Step 2: Select top features based on MI (l·∫•y t·∫•t c·∫£ nh·ªØng ch·ªâ s·ªë MI tr√™n 0.25)\n",
    "top_features = mi_df['Feature'].head(8).values ## C√≥ 10 ch·ªâ s·ªë MI tr√™n 0.25\n",
    "print(f\"Top features correlate to feature 'temp' based on MI: {top_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1. Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to keep\n",
    "features_to_keep = ['feelslike', 'tempmax', 'feelslikemax', 'tempmin', 'feelslikemin', 'dew',\n",
    "                     'sealevelpressure', 'humidity', 'temp']\n",
    "\n",
    "# Drop columns that are not in the list of features to keep\n",
    "raw_data = raw_data[features_to_keep]\n",
    "print(\"Dropped columns successfully. Feature to keeps: \", features_to_keep)\n",
    "# print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Split-training the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our regression problem, Stratified Sampling is not necessary because the features and target values are continuous rather than categorical. Stratified Sampling is typically used in classification problems to maintain the distribution of target classes across datasets. In regression, where the goal is to predict continuous values, maintaining class proportions is irrelevant and does not impact the model's performance. Therefore, I will focus on data normalization and model optimization to enhance prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_set, test_set = train_test_split(raw_data, test_size=0.2, random_state=42)  # Set random_state to get the same training set each time, \n",
    "                                                                                     # otherwise, when repeating training many times, your model might see all the data\n",
    "print('\\n____________ Split training and test set ____________')     \n",
    "print(len(train_set), \"training +\", len(test_set), \"test examples\")\n",
    "print(train_set.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Separate labels from data, since we do not process label values (already processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate labels from data\n",
    "X_train = train_set.drop(columns=['temp'])\n",
    "y_train = train_set['temp']\n",
    "X_test = test_set.drop(columns=['temp'])\n",
    "y_test = test_set['temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4. Define pipelines for processing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.1. Define ColumnSelector: a transformer for choosing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    # H√†m kh·ªüi t·∫°o nh·∫≠n v√†o danh s√°ch c√°c t√™n c·ªôt c·∫ßn ch·ªçn\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names  # L∆∞u tr·ªØ danh s√°ch t√™n c·ªôt\n",
    "\n",
    "    # Ph∆∞∆°ng th·ª©c fit kh√¥ng c·∫ßn th·ª±c hi·ªán g√¨, ch·ªâ tr·∫£ v·ªÅ ch√≠nh ƒë·ªëi t∆∞·ª£ng n√†y\n",
    "    # ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi quy tr√¨nh c·ªßa scikit-learn\n",
    "    def fit(self, dataframe, labels=None):\n",
    "        return self\n",
    "\n",
    "    # Ph∆∞∆°ng th·ª©c transform ch·ªçn c√°c c·ªôt t·ª´ DataFrame d·ª±a tr√™n danh s√°ch t√™n c·ªôt\n",
    "    # v√† tr·∫£ v·ªÅ c√°c gi√° tr·ªã d∆∞·ªõi d·∫°ng m·∫£ng NumPy\n",
    "    def transform(self, dataframe):\n",
    "        return dataframe[self.feature_names].values  # Ch·ªçn v√† tr·∫£ v·ªÅ c√°c c·ªôt d∆∞·ªõi d·∫°ng m·∫£ng NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical features\n",
    "num_feat_names = ['feelslike', 'tempmax', 'feelslikemax', 'tempmin', 'feelslikemin', 'dew', 'sealevelpressure', 'humidity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.2.Pipeline for categorical features (IN OUR TRAINING AND TESTING DATASET, THERE ARE NO CATEGORICAL FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.3. KH√îNG C·∫¶N L√ÄM B∆Ø·ªöC N√ÄY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.4. Pipeline for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for numerical features\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', ColumnSelector(num_feat_names)),  # Ch·ªçn c√°c c·ªôt numeric\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\")),  # ƒêi·ªÅn gi√° tr·ªã thi·∫øu b·∫±ng median\n",
    "    ('std_scaler', StandardScaler(with_mean=True, with_std=True))  # Chu·∫©n h√≥a v·ªÅ zero mean v√† unit variance\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.5 Run the pipeline to process training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_set_val = num_pipeline.fit_transform(train_set)\n",
    "\n",
    "print('\\n____________ Processed feature values ____________')\n",
    "print(processed_train_set_val[:3, :])  # In ra m·ªôt v√†i h√†ng ƒë·∫ßu ti√™n sau khi x·ª≠ l√Ω\n",
    "print(processed_train_set_val.shape)  # In ra k√≠ch th∆∞·ªõc c·ªßa d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\n",
    "joblib.dump(num_pipeline, r'models/num_pipeline.pkl')   # L∆∞u pipeline v√†o file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L∆ØU √ù: C√°c gi√° tr·ªã √¢m trong processed_train_set_val l√† ho√†n to√†n h·ª£p l√Ω v√† th∆∞·ªùng g·∫∑p trong c√°c b√†i to√°n h·ªçc m√°y, ƒë·∫∑c bi·ªát l√† khi b·∫°n s·ª≠ d·ª•ng chu·∫©n h√≥a (scaling) nh∆∞ StandardScaler. ƒê√¢y l√† m·ªôt s·ªë l√Ω do v√† c√°ch gi·∫£i th√≠ch:\n",
    "\n",
    "1. Chu·∫©n h√≥a (Standardization):\n",
    "StandardScaler: B·∫°n ƒë√£ s·ª≠ d·ª•ng StandardScaler trong pipeline c·ªßa m√¨nh. StandardScaler chu·∫©n h√≥a c√°c ƒë·∫∑c tr∆∞ng b·∫±ng c√°ch tr·ª´ gi√° tr·ªã trung b√¨nh v√† chia cho ƒë·ªô l·ªách chu·∫©n, ƒë·ªÉ c√°c ƒë·∫∑c tr∆∞ng c√≥ ph√¢n ph·ªëi chu·∫©n v·ªõi trung b√¨nh b·∫±ng 0 v√† ƒë·ªô l·ªách chu·∫©n b·∫±ng 1.\n",
    "K·∫øt qu·∫£: Sau khi chu·∫©n h√≥a, m·ªôt s·ªë gi√° tr·ªã c√≥ th·ªÉ √¢m, ƒëi·ªÅu n√†y ho√†n to√†n b√¨nh th∆∞·ªùng. N·∫øu m·ªôt gi√° tr·ªã c·ªßa ƒë·∫∑c tr∆∞ng th·∫•p h∆°n trung b√¨nh c·ªßa ƒë·∫∑c tr∆∞ng ƒë√≥, sau khi chu·∫©n h√≥a, gi√° tr·ªã ƒë√≥ c√≥ th·ªÉ √¢m.\n",
    "2. Gi√° tr·ªã √¢m trong D·ªØ li·ªáu:\n",
    "C·ªôt d·ªØ li·ªáu: N·∫øu d·ªØ li·ªáu g·ªëc c√≥ gi√° tr·ªã √¢m (v√≠ d·ª• nh∆∞ √°p su·∫•t kh√¥ng kh√≠ c√≥ th·ªÉ c√≥ gi√° tr·ªã √¢m trong m·ªôt s·ªë h·ªá th·ªëng ƒëo l∆∞·ªùng), th√¨ sau khi chu·∫©n h√≥a, c√°c gi√° tr·ªã ƒë√≥ c√≥ th·ªÉ v·∫´n c√≤n √¢m.\n",
    "3. √ù Nghƒ©a c·ªßa Gi√° tr·ªã √Çm:\n",
    "K√Ω hi·ªáu c·ªßa Chu·∫©n h√≥a: Gi√° tr·ªã √¢m trong k·∫øt qu·∫£ chu·∫©n h√≥a ch·ªâ ra r·∫±ng gi√° tr·ªã g·ªëc c·ªßa ƒë·∫∑c tr∆∞ng ƒë√≥ th·∫•p h∆°n trung b√¨nh c·ªßa ƒë·∫∑c tr∆∞ng ƒë√≥ trong t·∫≠p d·ªØ li·ªáu. ƒêi·ªÅu n√†y kh√¥ng c√≥ nghƒ©a l√† d·ªØ li·ªáu c√≥ v·∫•n ƒë·ªÅ, m√† ch·ªâ ƒë∆°n gi·∫£n l√† n√≥ ·ªü d∆∞·ªõi m·ª©c trung b√¨nh.\n",
    "4. V√≠ d·ª•:\n",
    "N·∫øu m·ªôt ƒë·∫∑c tr∆∞ng X c√≥ gi√° tr·ªã trung b√¨nh l√† 100 v√† ƒë·ªô l·ªách chu·∫©n l√† 15, th√¨ gi√° tr·ªã chu·∫©n h√≥a ƒë∆∞·ª£c t√≠nh theo c√¥ng th·ª©c: \n",
    "ùëç\n",
    "=\n",
    "ùëã\n",
    "‚àí\n",
    "mean\n",
    "std\n",
    "Z= \n",
    "std\n",
    "X‚àímean\n",
    "‚Äã\n",
    " \n",
    "\n",
    "N·∫øu X = 85, th√¨: \n",
    "ùëç\n",
    "=\n",
    "85\n",
    "‚àí\n",
    "100\n",
    "15\n",
    "=\n",
    "‚àí\n",
    "1\n",
    "Z= \n",
    "15\n",
    "85‚àí100\n",
    "‚Äã\n",
    " =‚àí1\n",
    "Gi√° tr·ªã chu·∫©n h√≥a -1 l√† h·ª£p l√Ω v√† c√≥ th·ªÉ x·∫£y ra.\n",
    "T√≥m l·∫°i:\n",
    "Gi√° tr·ªã √¢m trong processed_train_set_val l√† m·ªôt k·∫øt qu·∫£ b√¨nh th∆∞·ªùng c·ªßa vi·ªác chu·∫©n h√≥a d·ªØ li·ªáu v√† ph·∫£n √°nh s·ª± ph√¢n b·ªë d·ªØ li·ªáu g·ªëc so v·ªõi trung b√¨nh. ƒêi·ªÅu n√†y gi√∫p c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh h·ªçc m√°y b·∫±ng c√°ch ƒë∆∞a t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng v·ªÅ c√πng m·ªôt quy m√¥ v√† ph√¢n ph·ªëi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train and evaluate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Try Light GBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1.1. Training: Learn a lgbm model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2score_and_rmse(model, train_data, labels): \n",
    "    r2score = model.score(train_data, labels)\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    prediction = model.predict(train_data)\n",
    "    mse = mean_squared_error(labels, prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return r2score, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1.4 Store models to files, to compare latter\n",
    "#from sklearn.externals import joblib \n",
    "import joblib # new lib\n",
    "def store_model(model, model_name = \"\"):\n",
    "    # NOTE: sklearn.joblib faster than pickle of Python\n",
    "    # INFO: can store only ONE object in a file\n",
    "    if model_name == \"\": \n",
    "        model_name = type(model).__name__\n",
    "    joblib.dump(model,'models/' + model_name + '_model.pkl')\n",
    "    print(f\"Model successfully saved as \" + model_name + '_model.pkl')\n",
    "    \n",
    "def load_model(model_name):\n",
    "    # Load objects into memory\n",
    "    #del model\n",
    "    model = joblib.load('models/' + model_name + '_model.pkl')\n",
    "    #print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Try Light GBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor() #fix here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n____________ LGBMRegressor ____________')\n",
    "\n",
    "r2score, rmse = r2score_and_rmse(model, X_train, y_train)\n",
    "print('\\nR2 score (on training data, best=1):', r2score)\n",
    "print(\"Root Mean Square Error: \", rmse.round(decimals=1))\n",
    "\n",
    "\n",
    "# Predict labels for some test instances\n",
    "print(\"\\nPredictions: \", model.predict(X_test[:9]).round(decimals=1))\n",
    "print(\"Labels:      \", list(y_test[:9]))\n",
    "\n",
    "store_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Try XGB model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Try LTST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
